{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2425ad3-aabf-4af9-97e0-20aa2961611c",
   "metadata": {},
   "source": [
    "# Finetuning with OpenAI and Google\n",
    "\n",
    "This notebook trains a model that will create a line in the style of a Shakespearean sonnet when given a single word.\n",
    "\n",
    "See [./finetuning_0_dataset.ipynb](./finetuning_0_dataset.ipynb) for how this dataset was created.\n",
    "See [./indexed_sonnets.json](./indexed_sonnets.json) to create the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b59d7f-9a38-48ea-98ac-0f5099c2bab6",
   "metadata": {},
   "source": [
    "## Set up.\n",
    "\n",
    "Install the necessary packages, set up the API keys etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432f556f-e1ac-4cc4-b928-b45b25c5a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcb575b-9f6f-407b-9680-0f2ed2dabf0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../keys.env\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7140bb-8d4a-49f7-905a-19c5948d131e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many examples should we use?\n",
    "# There are about 6000 examples in the file and it will cost you about $10 to fine-tune OpenAI gpt-4o-mini on all of them.\n",
    "# So, choose the sampling percentage to reduce the cost.  By choosing 0.1 here, it will cost me approximately $1.\n",
    "# Specify 1.0 to train on all the samples\n",
    "SAMPLING = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de62c6f-807a-489b-a597-754eaff28589",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OpenAI\n",
    "\n",
    "Finetune a gpt-4o-mini model.\n",
    "\n",
    "### 1. Create datataset\n",
    "\n",
    "Following the instructions in https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de5aea5-c15c-448d-ab02-f4c6a486793e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"input\": \"creatures\",\n",
      "    \"output\": \"From fairest creatures we desire increase,\"\n",
      "  },\n",
      "  {\n",
      "    \"input\": \"desire\",\n",
      "    \"output\": \"From fairest creatures we desire increase,\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head indexed_sonnets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5774cd7b-af84-4ae2-bf3f-90a05f4a3b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'You are a chatbot takes a single word as input and writes a line of poetry that contains the given word.'},\n",
       "  {'role': 'user', 'content': 'famine'},\n",
       "  {'role': 'assistant', 'content': 'Making a famine where abundance lies,'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "messages = []\n",
    "SYSTEM_PROMPT = \"You are a chatbot takes a single word as input and writes a line of poetry that contains the given word.\"\n",
    "with open('indexed_sonnets.json') as ifp:\n",
    "    indexed_poems = json.load(ifp)\n",
    "    # required format:\n",
    "    # {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, \n",
    "    #               {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, \n",
    "    #               {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "    for poem in indexed_poems:\n",
    "        messages.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": poem['input']},\n",
    "                {\"role\": \"assistant\", \"content\": poem['output']}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "messages[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52b1d54-d626-47da-ac8b-fe57246fea55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6387"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc864b03-55fe-4851-a5a7-df1984b7d39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "with open('finetuning_openai.jsonl', 'w') as ofp:\n",
    "    for message in messages:\n",
    "        if random.random() < SAMPLING:\n",
    "            # write the message as a single line JSON\n",
    "            line = json.dumps(message)\n",
    "            ofp.write(line)\n",
    "            ofp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6677af25-a382-4819-a722-8420cbc6fb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622 finetuning_openai.jsonl\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a chatbot takes a single word as input and writes a line of poetry that contains the given word.\"}, {\"role\": \"user\", \"content\": \"desire\"}, {\"role\": \"assistant\", \"content\": \"From fairest creatures we desire increase,\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a chatbot takes a single word as input and writes a line of poetry that contains the given word.\"}, {\"role\": \"user\", \"content\": \"time\"}, {\"role\": \"assistant\", \"content\": \"But as the riper should by time decease,\"}]}\n"
     ]
    }
   ],
   "source": [
    "!wc -l finetuning_openai.jsonl            \n",
    "!head -2 finetuning_openai.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39186a86-4db1-4956-b695-fba1f041b1d3",
   "metadata": {},
   "source": [
    "## 2. Validate training file and estimate cost\n",
    "\n",
    "following instructions at https://cookbook.openai.com/examples/chat_finetuning_data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db542cf-0d11-45ca-990f-e40ebda2170c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 622\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a chatbot takes a single word as input and writes a line of poetry that contains the given word.'}\n",
      "{'role': 'user', 'content': 'desire'}\n",
      "{'role': 'assistant', 'content': 'From fairest creatures we desire increase,'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 45, 62\n",
      "mean / median: 50.60128617363344, 50.0\n",
      "p5 / p95: 48.0, 53.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 6, 22\n",
      "mean / median: 11.041800643086816, 11.0\n",
      "p5 / p95: 9.0, 13.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~31474 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~94422 tokens\n"
     ]
    }
   ],
   "source": [
    "# Copy-pasted from https://cookbook.openai.com/examples/chat_finetuning_data_prep\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "data_path = \"finetuning_openai.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "    \n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "    \n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "    \n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07902c7f-0561-441e-8629-7d05a21722f2",
   "metadata": {},
   "source": [
    "## 3. Upload training file and start training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66b1bfe-621d-460f-a8c1-f491a279e3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-vqVo75gjyXaz8OqxXIkz8tyx', bytes=172839, created_at=1723648774, filename='finetuning_openai.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "training_file = client.files.create(\n",
    "  file=open(\"finetuning_openai.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "training_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "546bdf1c-d4fa-4c0b-86b4-af4c3d8aab1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-vqVo75gjyXaz8OqxXIkz8tyx'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01f953c9-84c5-4352-ae5d-3b852e885f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL=\"gpt-3.5-turbo-0125\"\n",
    "#BASE_MODEL=\"gpt-4o-mini-2024-07-18\"\n",
    "training_job = client.fine_tuning.jobs.create(\n",
    "  training_file=training_file.id, \n",
    "  model=BASE_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d94699a-525d-459e-b277-8de39b25cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-FxvxRSxJaZnj5gUE7Qa98Xz9', created_at=1723649613, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-O9DnLrdeTxgprcSfnVHOMUK9', result_files=[], seed=1650444829, status='validating_files', trained_tokens=None, training_file='file-vqVo75gjyXaz8OqxXIkz8tyx', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)], object='list', has_more=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "client.fine_tuning.jobs.list(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe731319-89a5-4344-aa62-7ec50a0a77b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-FxvxRSxJaZnj5gUE7Qa98Xz9', created_at=1723649613, error=Error(code='exceeded_quota', message='Creating this fine-tuning job would exceed your hard limit, please check your plan and billing details.                     Cost of job ftjob-FxvxRSxJaZnj5gUE7Qa98Xz9: USD 0.73. Quota remaining for org-O9DnLrdeTxgprcSfnVHOMUK9: USD -186.42.', param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-O9DnLrdeTxgprcSfnVHOMUK9', result_files=[], seed=1650444829, status='failed', trained_tokens=None, training_file='file-vqVo75gjyXaz8OqxXIkz8tyx', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_job_id='ftjob-FxvxRSxJaZnj5gUE7Qa98Xz9'  #(training_job.id)\n",
    "client.fine_tuning.jobs.retrieve(training_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ea1f7-f811-429f-9d2a-97ec570df330",
   "metadata": {},
   "source": [
    "## 4. Try out the finetuned model\n",
    "\n",
    "Make sure to supply messages in the same format used to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7890e0af-9a8c-4e53-a935-46888375bb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.retrieve(training_job_id).fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687e700-fd63-4b1d-b357-d2bfa7aab7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kitchen_input = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"kitchen\"},\n",
    "]\n",
    "kitchen_joke = client.chat.completions.create(\n",
    "  model=client.fine_tuning.jobs.retrieve(training_job.id).fine_tuned_model,\n",
    "  messages=kitchen_input\n",
    ")\n",
    "print(kitchen_joke.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c05762-74ff-49e7-b7ec-4df9e5d6a6fb",
   "metadata": {},
   "source": [
    "# Google Gemini\n",
    "\n",
    "Fine-tune a Gemini model following the instructions at:\n",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "155df324-b3ff-4007-ae1e-71c137ae0a64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'You are a chatbot takes a single word as input and writes a line of poetry that contains the given word.'},\n",
       "  {'role': 'user', 'content': 'famine'},\n",
       "  {'role': 'model', 'content': 'Making a famine where abundance lies,'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "messages = []\n",
    "\n",
    "with open('indexed_sonnets.json') as ifp:\n",
    "    indexed_poems = json.load(ifp)\n",
    "    # required format:\n",
    "    # {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, \n",
    "    #               {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, \n",
    "    #               {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "    for poem in indexed_poems:\n",
    "        messages.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": poem['input']},\n",
    "                {\"role\": \"model\", \"content\": poem['output']}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "messages[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "865742b6-f9ba-4e84-a19c-fcbb209d06d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "with open('finetuning_gemini.jsonl', 'w') as ofp:\n",
    "    for message in messages:\n",
    "        if random.random() < SAMPLING:\n",
    "            # write the message as a single line JSON\n",
    "            line = json.dumps(message)\n",
    "            ofp.write(line)\n",
    "            ofp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b863d8fc-5079-4095-8af5-9ad588c5064e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://finetuning_gemini.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][166.4 KiB/166.4 KiB]                                                \n",
      "Operation completed over 1 objects/166.4 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "BUCKET=\"viz_genai_nonsensitive\"  # CHANGE THIS to be your own bucket\n",
    "REGION=\"us-central1\"\n",
    "!gsutil cp finetuning_gemini.jsonl gs://$BUCKET/finetuning_gemini.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4a08e0e-fcb5-4f6c-a027-dd0b48011690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95265715-08c9-42dd-bf0d-50afd23e0e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT=!gcloud config get project\n",
    "PROJECT=PROJECT[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f8d005c-7a20-416e-b80d-ce3299f0c567",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SupervisedTuningJob\n",
      "SupervisedTuningJob created. Resource name: projects/82379820716/locations/us-central1/tuningJobs/2736685987722690560\n",
      "To use this SupervisedTuningJob in another session:\n",
      "tuning_job = sft.SupervisedTuningJob('projects/82379820716/locations/us-central1/tuningJobs/2736685987722690560')\n",
      "View Tuning Job:\n",
      "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/2736685987722690560?project=82379820716\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-1853c827-e7b4-44bf-ac9c-5bdaec7ad1ac\" href=\"#view-view-vertex-resource-1853c827-e7b4-44bf-ac9c-5bdaec7ad1ac\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-1853c827-e7b4-44bf-ac9c-5bdaec7ad1ac');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20240813174030386875/runs?project=silverlake-data-science-team');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20240813174030386875/runs?project=silverlake-data-science-team', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/82379820716/locations/us-central1/models/7757618583424729088@1\n",
      "projects/82379820716/locations/us-central1/endpoints/9031474272459030528\n",
      "<google.cloud.aiplatform.metadata.experiment_resources.Experiment object at 0x7fdfd437f130>\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.tuning import sft\n",
    "import time\n",
    "sft_tuning_job = sft.train(\n",
    "    source_model=\"gemini-1.0-pro-002\",\n",
    "    train_dataset=f\"gs://{BUCKET}/finetuning_gemini.jsonl\"\n",
    ")\n",
    "\n",
    "# Polling for job completion\n",
    "while not sft_tuning_job.has_ended:\n",
    "    time.sleep(60)\n",
    "    sft_tuning_job.refresh()\n",
    "\n",
    "print(sft_tuning_job.tuned_model_name)\n",
    "print(sft_tuning_job.tuned_model_endpoint_name)\n",
    "print(sft_tuning_job.experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22644edb-476d-4a13-9bce-c9d106777202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"Kitchen and chapel, have he still at peace\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.0697949230670929\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.05252167582511902\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.07949569821357727\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.03422932326793671\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.12896329164505005\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.04611974582076073\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.2628418207168579\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.08525123447179794\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 31\n",
      "  candidates_token_count: 9\n",
      "  total_token_count: 40\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from vertexai.preview import tuning\n",
    "from vertexai.preview.tuning import sft\n",
    "\n",
    "tuned_model = GenerativeModel('projects/82379820716/locations/us-central1/endpoints/9031474272459030528') #sft_tuning_job.tuned_model_endpoint_name)\n",
    "print(tuned_model.generate_content(f\"{SYSTEM_PROMPT}\\n  User: kitchen  Model:\\n\"))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
